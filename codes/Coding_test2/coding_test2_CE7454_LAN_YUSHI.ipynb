{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE7454 : Deep Learning for Data Science\n",
    "##Â Xavier Bresson\n",
    "\n",
    "\n",
    "## Coding Test 2\n",
    "Date: November 25th, 2020<br>\n",
    "\n",
    "*Instructions* <br>\n",
    "Name: Do not forget to add your name to the notebook file \"coding_test2_CE7454_YOUR_NAME.ipynb\".<br>\n",
    "Questions: This notebook has 10 questions.<br>\n",
    "Answers: Write the answers to each question in this notebook.<br>\n",
    "Type: This test is individual and open-book.<br>\n",
    "Grading: 1 point for each question.<br>\n",
    "Output/Timestamp: There is no point if the code has no output (the cell was not executed) or the timestamp is beyond **6:15pm**.<br>\n",
    "Delivery: Upload your notebook to https://drive.google.com/drive/folders/1k2EliAl799GVwk6ra5wJhpSdmS58TGrb  by **6:20pm**. <br>\n",
    "Remark: **If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want.**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TT3HcmXNhNxp"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Given a matrix $x=[[2,1],[4,8]]$. Compute the inverse matrix $x^{-1}$ and print it. Compute and print the matrix product of $x^{-1}$ and $x$.\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1600109702676,
     "user": {
      "displayName": "Vijay Prakash Dwivedi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgfLL5UlJ0cWGCM7ABRbVFcbAS9vkLq7ias9ewKNA=s64",
      "userId": "03190496352220804755"
     },
     "user_tz": -480
    },
    "id": "vH-tgwa9gf4L",
    "outputId": "22855a52-17af-4e45-d011-3edc6d8f5fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-11-25--16-59-26\n",
      "inverse matrix:\n",
      "tensor([[ 0.6667, -0.0833],\n",
      "        [-0.3333,  0.1667]])\n",
      "\n",
      "Mat multplication:\n",
      "tensor([[ 1.0000e+00, -1.4901e-08],\n",
      "        [ 0.0000e+00,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "x = torch.Tensor([[2,1],[4,8]])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print('inverse matrix:')\n",
    "print(torch.inverse(x))\n",
    "print()\n",
    "print('Mat multplication:')\n",
    "print(x@torch.inverse(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "\n",
    "Given a minibatch $x$ of color images represented by a tensor of size (100, 3, 28, 28), with 3 color channels (red, green and blue), a grid domain of 28 pixels by 28 pixels, and 100 images in the minibatch. \n",
    "\n",
    "Reduce the size of the tensor $x$ to (100, 3, 7, 7) with a max pooling operator. Print the size of $x$ for confirmation.\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-11-25--17-01-27\n",
      "torch.Size([100, 3, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "x = torch.rand(100,3,28,28) # given minibatch of color images \n",
    "\n",
    "# YOUR CODE HERE\n",
    "pool  = torch.nn.MaxPool2d(4,4)\n",
    "x_pool = pool(x)\n",
    "print(x_pool.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Given a linear layer with 10 input neurons and 100 output neurons and no bias, initialize the parameter matrix $W\\in\\mathbb{R}^{100\\times 10}$ with random values and such that the standard deviation of $W$ is 1, i.e. std($W$)=1. Print the standard deviation of $W$ for confirmation.\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-11-25--17-06-06\n",
      "std: tensor(1.0029, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "# w = torch.empty(3, 5)\n",
    "linear = torch.nn.Linear(10, 100, bias=False)\n",
    "torch.nn.init.normal_(linear.weight)\n",
    "print('std:', torch.std(linear.weight))\n",
    "# print('mean:', torch.mean(linear.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Given the function $f=xy+(xy)^2$, compute and print the gradients of $f$ with respect to $x$ and $y$ for $x=2$ and $y=1$ using the *backward()* Pytorch function. \n",
    "\n",
    "Possible hint: Function *.grad*\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-11-25--19-05-05\n",
      "x.grad: tensor([5.])\n",
      "y.grad: tensor([10.])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def f(x,y):\n",
    "    return x*y+(x*y)**2\n",
    "x=torch.Tensor([2])\n",
    "y=torch.Tensor([1])\n",
    "x.requires_grad=True\n",
    "y.requires_grad=True\n",
    "res = f(x,y)\n",
    "res.backward()\n",
    "# res.grad\n",
    "print('x.grad:', x.grad)\n",
    "print('y.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Give a simple PyTorch example using a tensor where *detach()* is used? \n",
    "\n",
    "Give a simple PyTorch example using a tensor where *'requires_grad'=False* is used? \n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-11-25--17-18-27\n",
      "detached tensor has no grad: False\n",
      "no grad on w if b has no grad: False\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# detach\n",
    "a = torch.tensor([1,2,3.], requires_grad=True)\n",
    "# are detached tensor's leafs? yes they are\n",
    "a_detached = a.detach()\n",
    "print('detached tensor has no grad:', a_detached.requires_grad)\n",
    "# requires_grad=False\n",
    "b = torch.tensor([1,2,3.], requires_grad=False)\n",
    "w = torch.tensor([5]) * b\n",
    "print('no grad on w if b has no grad:',w.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Given the definition of the LeakyReLU function $\\sigma$ :\n",
    "\n",
    "$$ \\sigma(x) = \n",
    "\\left\\{\n",
    "\\begin{array}{lll}\n",
    "x &\\textrm{ for } x\\geq 0\\\\\n",
    "\\alpha x &\\textrm{ for } x< 0\n",
    "\\end{array}\n",
    "\\right., \\quad \\textrm{ with } \\alpha=0.1.\n",
    "$$\n",
    "\n",
    "Implement the forward and the backward functions explicitly in Pytorch. \n",
    "\n",
    "Print the output value $y=\\sigma(x)$ for the input tensor $x=[-1.2]$. Print the gradient value of $y$ with respect to $x$ for $x=[-1.2]$.\n",
    "\n",
    "Possible hints: https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html<br>\n",
    "Functions *.backward()* and *.grad*\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-11-25--17-30-27\n",
      "tensor(-0.1200, grad_fn=<LeakyReLUBackward>)\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "x = torch.tensor(-1.2, requires_grad=True)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class LeakyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "#     def __init__(self, a=0.1):\n",
    "#         super(LeakyReLU, self).__init__()\n",
    "#         self.a=a\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        if input>=0:\n",
    "            return input\n",
    "        else:\n",
    "            return 0.1 * input\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] *= 0.1\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# myReLU = LeakyReLU(a=0.1)\n",
    "# myReLU(x)\n",
    "output = LeakyReLU.apply(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Train a convolutional neural network with two convolutional layers and two linear layers on a dataset composed of a subset of MNIST with 1000 training images and 500 test images.\n",
    "\n",
    "The image domain will be reduced by a factor 2 after each convolutional layer with the use of the average pooling operator implemented in PyTorch with function *nn.AvgPool2d*.\n",
    "\n",
    "Print the test error. The test error must reach a value smaller than 25%. \n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-11-25--17-54-56\n",
      "torch.Size([1000, 28, 28])\n",
      "torch.Size([500, 28, 28])\n",
      " \n",
      "epoch= 0 \t time= 0.3408181667327881 \t loss= 2.2935461044311523 \t error= 84.7000002861023 percent\n",
      "test error  =  84.20000076293945 percent\n",
      " \n",
      "epoch= 5 \t time= 1.7618813514709473 \t loss= 2.2355207920074465 \t error= 62.90000081062317 percent\n",
      "test error  =  61.799999475479126 percent\n",
      " \n",
      "epoch= 10 \t time= 3.120983600616455 \t loss= 2.127175736427307 \t error= 47.80000030994415 percent\n",
      "test error  =  45.59999942779541 percent\n",
      " \n",
      "epoch= 15 \t time= 4.622917890548706 \t loss= 1.888116729259491 \t error= 39.00000035762787 percent\n",
      "test error  =  36.39999985694885 percent\n",
      " \n",
      "epoch= 20 \t time= 6.15389609336853 \t loss= 1.4445422530174254 \t error= 30.200000405311584 percent\n",
      "test error  =  30.800001621246338 percent\n",
      " \n",
      "epoch= 25 \t time= 7.571792125701904 \t loss= 0.9866807043552399 \t error= 22.000000476837158 percent\n",
      "test error  =  27.199999094009396 percent\n",
      " \n",
      "epoch= 30 \t time= 8.962189197540283 \t loss= 0.73006312251091 \t error= 19.09999966621399 percent\n",
      "test error  =  25.0 percent\n",
      " \n",
      "epoch= 35 \t time= 10.477903366088867 \t loss= 0.5933240592479706 \t error= 16.399999856948853 percent\n",
      "test error  =  21.80000066757202 percent\n",
      " \n",
      "epoch= 40 \t time= 11.91471815109253 \t loss= 0.5203623622655869 \t error= 15.399999618530275 percent\n",
      "test error  =  20.600000619888306 percent\n",
      " \n",
      "epoch= 45 \t time= 13.408584117889404 \t loss= 0.46924138963222506 \t error= 13.00000011920929 percent\n",
      "test error  =  18.000000715255737 percent\n",
      " \n",
      "epoch= 50 \t time= 14.842692613601685 \t loss= 0.43384193778038027 \t error= 13.100000619888306 percent\n",
      "test error  =  17.200000286102295 percent\n",
      " \n",
      "epoch= 55 \t time= 16.310149669647217 \t loss= 0.4044014662504196 \t error= 11.19999885559082 percent\n",
      "test error  =  16.00000023841858 percent\n",
      " \n",
      "epoch= 60 \t time= 17.725151300430298 \t loss= 0.384464767575264 \t error= 11.099998354911804 percent\n",
      "test error  =  16.00000023841858 percent\n",
      " \n",
      "epoch= 65 \t time= 19.145262956619263 \t loss= 0.3709775507450104 \t error= 10.59999942779541 percent\n",
      "test error  =  16.200000047683716 percent\n",
      " \n",
      "epoch= 70 \t time= 20.79796838760376 \t loss= 0.3528903856873512 \t error= 10.799999833106995 percent\n",
      "test error  =  16.00000023841858 percent\n",
      " \n",
      "epoch= 75 \t time= 22.363919973373413 \t loss= 0.3389144062995911 \t error= 9.999999403953552 percent\n",
      "test error  =  15.399999618530275 percent\n",
      " \n",
      "epoch= 80 \t time= 23.87221336364746 \t loss= 0.3276471048593521 \t error= 9.999999403953552 percent\n",
      "test error  =  14.399999380111694 percent\n",
      " \n",
      "epoch= 85 \t time= 25.599351406097412 \t loss= 0.3147552698850632 \t error= 9.500000476837158 percent\n",
      "test error  =  15.600000619888304 percent\n",
      " \n",
      "epoch= 90 \t time= 27.251432180404663 \t loss= 0.30532667189836504 \t error= 9.100000262260437 percent\n",
      "test error  =  14.600000381469727 percent\n",
      " \n",
      "epoch= 95 \t time= 28.66330099105835 \t loss= 0.295806485414505 \t error= 8.699999451637268 percent\n",
      "test error  =  14.799998998641966 percent\n",
      " \n",
      "epoch= 100 \t time= 30.28201985359192 \t loss= 0.2834719017148018 \t error= 7.999999523162842 percent\n",
      "test error  =  15.199999809265135 percent\n",
      " \n",
      "epoch= 105 \t time= 31.75875210762024 \t loss= 0.27675279378890993 \t error= 8.799999952316284 percent\n",
      "test error  =  14.800000190734863 percent\n",
      " \n",
      "epoch= 110 \t time= 33.23004412651062 \t loss= 0.26376847326755526 \t error= 8.899999260902405 percent\n",
      "test error  =  14.799998998641966 percent\n",
      " \n",
      "epoch= 115 \t time= 34.86108660697937 \t loss= 0.2610718861222267 \t error= 7.999999523162842 percent\n",
      "test error  =  15.600000619888304 percent\n",
      " \n",
      "epoch= 120 \t time= 36.29099488258362 \t loss= 0.2536169081926346 \t error= 7.5999999046325675 percent\n",
      "test error  =  13.999999761581423 percent\n",
      " \n",
      "epoch= 125 \t time= 37.77911067008972 \t loss= 0.242645563185215 \t error= 7.1999990940093985 percent\n",
      "test error  =  13.999999761581423 percent\n",
      " \n",
      "epoch= 130 \t time= 39.1517128944397 \t loss= 0.23669951558113098 \t error= 6.999999880790711 percent\n",
      "test error  =  13.200000524520874 percent\n",
      " \n",
      "epoch= 135 \t time= 40.55804681777954 \t loss= 0.2303960293531418 \t error= 7.799999713897705 percent\n",
      "test error  =  13.200000524520874 percent\n",
      " \n",
      "epoch= 140 \t time= 42.08161497116089 \t loss= 0.22266806438565254 \t error= 7.199999690055847 percent\n",
      "test error  =  14.400000572204592 percent\n",
      " \n",
      "epoch= 145 \t time= 43.741267681121826 \t loss= 0.22347190827131272 \t error= 6.799999475479127 percent\n",
      "test error  =  13.999999761581423 percent\n",
      " \n",
      "epoch= 150 \t time= 45.201783657073975 \t loss= 0.2039985828101635 \t error= 6.300000548362732 percent\n",
      "test error  =  13.799999952316282 percent\n",
      " \n",
      "epoch= 155 \t time= 46.643534421920776 \t loss= 0.196636875718832 \t error= 5.6999993324279785 percent\n",
      "test error  =  11.999999284744263 percent\n",
      " \n",
      "epoch= 160 \t time= 48.13343667984009 \t loss= 0.1951953575015068 \t error= 6.299999952316285 percent\n",
      "test error  =  14.200000762939451 percent\n",
      " \n",
      "epoch= 165 \t time= 49.575671434402466 \t loss= 0.18879542946815492 \t error= 6.200000047683716 percent\n",
      "test error  =  13.200000524520874 percent\n",
      " \n",
      "epoch= 170 \t time= 50.99469566345215 \t loss= 0.18286956399679183 \t error= 5.600000023841858 percent\n",
      "test error  =  13.199999332427979 percent\n",
      " \n",
      "epoch= 175 \t time= 52.43007779121399 \t loss= 0.1738505981862545 \t error= 5.59999942779541 percent\n",
      "test error  =  12.799999713897705 percent\n",
      " \n",
      "epoch= 180 \t time= 54.06561732292175 \t loss= 0.1744542747735977 \t error= 4.800000190734863 percent\n",
      "test error  =  12.999998331069945 percent\n",
      " \n",
      "epoch= 185 \t time= 55.57099533081055 \t loss= 0.16863586902618408 \t error= 4.899999499320984 percent\n",
      "test error  =  12.59999990463257 percent\n",
      " \n",
      "epoch= 190 \t time= 56.9378399848938 \t loss= 0.1592307984828949 \t error= 4.699999690055847 percent\n",
      "test error  =  12.799999713897705 percent\n",
      " \n",
      "epoch= 195 \t time= 58.44821238517761 \t loss= 0.1581991922110319 \t error= 4.499999284744263 percent\n",
      "test error  =  12.600001096725464 percent\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "def get_error(scores, labels):\n",
    "    bs=scores.size(0)\n",
    "    predicted_labels = scores.argmax(dim=1)\n",
    "    indicator = (predicted_labels == labels)\n",
    "    num_matches=indicator.sum()\n",
    "    return 1-num_matches.float()/bs  \n",
    "\n",
    "[train_data,train_label,test_data,test_label] = torch.load('small_MNIST.pt')\n",
    "print(train_data.size())\n",
    "print(test_data.size())\n",
    "\n",
    "lr = 0.25 # possible value but you may choose any value you want\n",
    "bs = 100  # possible value but you may choose any value you want\n",
    "import torch.nn as nn\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def eval_on_test_set():\n",
    "\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    for i in range(0,500,bs):\n",
    "\n",
    "        minibatch_data =  test_data[i:i+bs]\n",
    "        minibatch_label= test_label[i:i+bs]\n",
    "\n",
    "#         inputs = minibatch_data.view(bs,784)\n",
    "        inputs = minibatch_data.view(bs, 1, 28, 28)\n",
    "\n",
    "        scores=net( inputs ) \n",
    "\n",
    "        error = get_error( scores , minibatch_label)\n",
    "\n",
    "        running_error += error.item()\n",
    "\n",
    "        num_batches+=1\n",
    "\n",
    "\n",
    "    total_error = running_error/num_batches\n",
    "    print( 'test error  = ', total_error*100 ,'percent')\n",
    "\n",
    "class two_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=3136, hidden_size=50,  output_size=10):\n",
    "        super(two_layer_net , self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,   64,  kernel_size=3, padding=1 )\n",
    "        self.pool1  = nn.AvgPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(64,  64,  kernel_size=3, padding=1 )\n",
    "        self.pool2  = nn.AvgPool2d(2,2)\n",
    "        self.layer1 = nn.Linear(  input_size   , hidden_size  , bias=False  )\n",
    "        self.layer2 = nn.Linear(  hidden_size  , output_size   , bias=False  )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs, -1)\n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = F.relu(y)\n",
    "        scores  = self.layer2(y_hat)\n",
    "        \n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = F.relu(y)\n",
    "        scores  = self.layer2(y_hat)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "from utils import check_mnist_dataset_exists\n",
    "data_path=check_mnist_dataset_exists()\n",
    "\n",
    "# load MNIST\n",
    "# train_data=torch.load(data_path+'mnist/train_data.pt')\n",
    "# train_label=torch.load(data_path+'mnist/train_label.pt')\n",
    "# test_data=torch.load(data_path+'mnist/test_data.pt')\n",
    "# test_label=torch.load(data_path+'mnist/test_label.pt')\n",
    "\n",
    "net=two_layer_net(3136,50,10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD( net.parameters() , lr=0.01 )\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(200):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    \n",
    "    shuffled_indices=torch.randperm(1000)\n",
    " \n",
    "    for count in range(0,1000,bs):\n",
    "        \n",
    "        # forward and backward pass\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        indices=shuffled_indices[count:count+bs]\n",
    "        minibatch_data =  train_data[indices]\n",
    "        minibatch_label= train_label[indices]\n",
    "\n",
    "#         inputs = minibatch_data.view(bs,784)\n",
    "#         inputs = minibatch_data\n",
    "        inputs = minibatch_data.view(bs, 1, 28, 28)\n",
    "\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        scores=net( inputs ) \n",
    "\n",
    "        loss =  criterion( scores , minibatch_label) \n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # compute some stats\n",
    "        \n",
    "        running_loss += loss.detach().item()\n",
    "               \n",
    "        error = get_error( scores.detach() , minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches+=1\n",
    "    \n",
    "    \n",
    "    # once the epoch is finished we divide the \"running quantities\"\n",
    "    # by the number of batches\n",
    "    \n",
    "    total_loss = running_loss/num_batches\n",
    "    total_error = running_error/num_batches\n",
    "    elapsed_time = time.time() - start\n",
    "    \n",
    "    # every 10 epoch we display the stats \n",
    "    # and compute the error rate on the test set  \n",
    "    \n",
    "    if epoch % 5 == 0 : \n",
    "    \n",
    "        print(' ')\n",
    "        \n",
    "        print('epoch=',epoch, '\\t time=', elapsed_time,\n",
    "              '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "        \n",
    "        eval_on_test_set()\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Train a recurrent neural network on the small PTB dataset using the PyTorch GRU layer:\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru#torch.nn.GRU\n",
    "\n",
    "The small PTB dataset is a sub-set of PTB composed of 20 sub-documents and each document has 1000 words.\n",
    "\n",
    "Print the perplexity, defined as exp(cross_entropy_loss). The perplexity must reach a value smaller than 600 for the training set. \n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-11-25--18-04-15\n",
      "torch.Size([1000, 20])\n",
      "torch.Size([1000, 20])\n",
      "torch.Size([1000, 20])\n",
      "\n",
      "epoch= 0 \t time= 0.6229805946350098 \t lr= 1 \t exp(loss)= 1483.72305482022\n",
      "test: exp(loss) =  823.1376400710045\n",
      "\n",
      "epoch= 1 \t time= 1.4386451244354248 \t lr= 1 \t exp(loss)= 701.2691196272631\n",
      "test: exp(loss) =  811.0872360189891\n",
      "\n",
      "epoch= 2 \t time= 2.2565202713012695 \t lr= 1 \t exp(loss)= 495.7053087830064\n",
      "test: exp(loss) =  686.3359186218908\n",
      "\n",
      "epoch= 3 \t time= 3.0750508308410645 \t lr= 1 \t exp(loss)= 395.68523015311337\n",
      "test: exp(loss) =  629.9823478018742\n",
      "\n",
      "epoch= 4 \t time= 3.935279607772827 \t lr= 0.9090909090909091 \t exp(loss)= 305.3454623519807\n",
      "test: exp(loss) =  564.4843039402205\n",
      "\n",
      "epoch= 5 \t time= 4.781304836273193 \t lr= 0.8264462809917354 \t exp(loss)= 245.555143649092\n",
      "test: exp(loss) =  561.4175079468112\n",
      "\n",
      "epoch= 6 \t time= 5.633679389953613 \t lr= 0.7513148009015777 \t exp(loss)= 202.95450271212474\n",
      "test: exp(loss) =  544.4533991276342\n",
      "\n",
      "epoch= 7 \t time= 6.489692211151123 \t lr= 0.6830134553650705 \t exp(loss)= 170.91909647397816\n",
      "test: exp(loss) =  535.3590136329126\n",
      "\n",
      "epoch= 8 \t time= 7.325531721115112 \t lr= 0.6209213230591549 \t exp(loss)= 146.27137637732514\n",
      "test: exp(loss) =  531.5410942879005\n",
      "\n",
      "epoch= 9 \t time= 8.238977432250977 \t lr= 0.5644739300537771 \t exp(loss)= 127.01961227494623\n",
      "test: exp(loss) =  529.6041018433014\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "[train_data,test_data] = torch.load('small_PTB.pt')\n",
    "print(train_data.size())\n",
    "\n",
    "\n",
    "\n",
    "seq_length = 20   # possible value but you may choose any value you want\n",
    "hidden_size = 150 # possible value but you may choose any value you want\n",
    "my_lr = 1            # possible value but you may choose any value you want\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# from utils import check_ptb_dataset_exists\n",
    "device= torch.device(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#device= torch.device(\"cpu\")\n",
    "print(  train_data.size()  )\n",
    "print(  test_data.size()   )\n",
    "\n",
    "bs = 20\n",
    "vocab_size = 10000\n",
    "\n",
    "class three_layer_recurrent_net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(three_layer_recurrent_net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = nn.GRU(       hidden_size , hidden_size  )\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
    "\n",
    "        \n",
    "    def forward(self, word_seq, h_init ):\n",
    "        \n",
    "        g_seq               =   self.layer1( word_seq )  \n",
    "        h_seq , h_final     =   self.layer2( g_seq , h_init )\n",
    "        score_seq           =   self.layer3( h_seq )\n",
    "        \n",
    "        return score_seq,  h_final \n",
    "\n",
    "hidden_size=150\n",
    "\n",
    "net = three_layer_recurrent_net( hidden_size )\n",
    "net = net.to(device)\n",
    "\n",
    "def normalize_gradient(net):\n",
    "\n",
    "    grad_norm_sq=0\n",
    "\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "   \n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('grad norm close to zero')\n",
    "    else:    \n",
    "        for p in net.parameters():\n",
    "             p.grad.data.div_(grad_norm)\n",
    "\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    \n",
    "    h=h.to(device)\n",
    "\n",
    "       \n",
    "    for count in range( 0 , 1000-seq_length ,  seq_length) :\n",
    "               \n",
    "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "        \n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "                                  \n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "        \n",
    "        loss = criterion(  scores ,  minibatch_label )    \n",
    "        \n",
    "        h=h.detach()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "        \n",
    "start=time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "    if epoch >= 4:\n",
    "        my_lr = my_lr / 1.1\n",
    "    \n",
    "    # create a new optimizer and give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h to be the zero vector\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "    # send it to the gpu    \n",
    "    h=h.to(device)\n",
    "    \n",
    "    for count in range( 0 , 1000-seq_length ,  seq_length):\n",
    "             \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # Detach to prevent from backpropagating all the way to the beginning\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.detach()\n",
    "        h=h.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    eval_on_test_set() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Replace the GRU layer in Question 8 with a transformer layer defined as :\n",
    "\n",
    "$$\n",
    "h_\\textrm{output} = \\textrm{softmax} \\Big( \\frac{QK^T}{\\sqrt{d}} \\Big) V \\\\\n",
    "Q = W_Q h_\\textrm{input} + b_Q \\\\\n",
    "K = W_K h_\\textrm{input} + b_K \\\\\n",
    "V = W_V h_\\textrm{input} + b_V \\\\\n",
    "$$\n",
    "\n",
    "where $h_\\textrm{input}, h_\\textrm{output}, Q, K, V$ are tensors of size (bs,len,d) with bs being the batch size, len being the sequence length and d the feature dimension.\n",
    "\n",
    "Train the network on the small PTB dataset as in Question 8. Print the perplexity, defined as exp(cross_entropy_loss). The perplexity must reach a value smaller than 600 for the training set. \n",
    "\n",
    "Possible hints: Function *torch.bmm*\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-11-25--18-14-39\n",
      "Timestamp: 20-11-25--18-14-40\n",
      "torch.Size([1000, 20])\n",
      "torch.Size([1000, 20])\n",
      "torch.Size([1000, 20])\n",
      "\n",
      "epoch= 0 \t time= 0.5959103107452393 \t lr= 0.25 \t exp(loss)= 2942.2731532715748\n",
      "test: exp(loss) =  1170.5025255282628\n",
      "\n",
      "epoch= 1 \t time= 1.3946542739868164 \t lr= 0.25 \t exp(loss)= 927.0728323832203\n",
      "test: exp(loss) =  871.9438398025994\n",
      "\n",
      "epoch= 2 \t time= 2.1597859859466553 \t lr= 0.25 \t exp(loss)= 732.67554710876\n",
      "test: exp(loss) =  776.6933925313\n",
      "\n",
      "epoch= 3 \t time= 2.9834177494049072 \t lr= 0.25 \t exp(loss)= 637.8703298776427\n",
      "test: exp(loss) =  724.4560324219872\n",
      "\n",
      "epoch= 4 \t time= 3.8042423725128174 \t lr= 0.22727272727272727 \t exp(loss)= 573.8999359870154\n",
      "test: exp(loss) =  690.5619256305843\n",
      "\n",
      "epoch= 5 \t time= 4.576582908630371 \t lr= 0.20661157024793386 \t exp(loss)= 529.1529979250217\n",
      "test: exp(loss) =  666.5005392017192\n",
      "\n",
      "epoch= 6 \t time= 5.347588777542114 \t lr= 0.1878287002253944 \t exp(loss)= 495.35695748366385\n",
      "test: exp(loss) =  647.9746787739394\n",
      "\n",
      "epoch= 7 \t time= 6.1152403354644775 \t lr= 0.17075336384126763 \t exp(loss)= 468.4554516258753\n",
      "test: exp(loss) =  633.0160369069998\n",
      "\n",
      "epoch= 8 \t time= 6.88761305809021 \t lr= 0.15523033076478873 \t exp(loss)= 446.3154699671816\n",
      "test: exp(loss) =  620.808656594276\n",
      "\n",
      "epoch= 9 \t time= 7.65256142616272 \t lr= 0.14111848251344428 \t exp(loss)= 427.71006133189746\n",
      "test: exp(loss) =  610.8350724625067\n",
      "\n",
      "epoch= 10 \t time= 8.470964908599854 \t lr= 0.1282895295576766 \t exp(loss)= 411.8576477108367\n",
      "test: exp(loss) =  602.5573865881341\n",
      "\n",
      "epoch= 11 \t time= 9.235490083694458 \t lr= 0.11662684505243327 \t exp(loss)= 398.21066888637694\n",
      "test: exp(loss) =  595.5374240885865\n",
      "\n",
      "epoch= 12 \t time= 10.006342887878418 \t lr= 0.10602440459312115 \t exp(loss)= 386.36529515419323\n",
      "test: exp(loss) =  589.4934212165848\n",
      "\n",
      "epoch= 13 \t time= 10.783250331878662 \t lr= 0.09638582235738286 \t exp(loss)= 376.01498938286846\n",
      "test: exp(loss) =  584.2456872437772\n",
      "\n",
      "epoch= 14 \t time= 11.561145782470703 \t lr= 0.08762347487034804 \t exp(loss)= 366.9216849605124\n",
      "test: exp(loss) =  579.6683195875145\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "# [train_data,test_data] = torch.load('small_PTB.pt')\n",
    "# print(train_data.size())\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "%reset -f\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "\n",
    "seq_length = 25   # possible value but you may choose any value you want\n",
    "hidden_size = 150 # possible value but you may choose any value you want\n",
    "my_lr = 0.25         # possible value but you may choose any value you want\n",
    "\n",
    "[train_data,test_data] = torch.load('small_PTB.pt')\n",
    "print(train_data.size())\n",
    "bs = 20\n",
    "vocab_size = 10000\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# from utils import check_ptb_dataset_exists\n",
    "device= torch.device(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#device= torch.device(\"cpu\")\n",
    "print(  train_data.size()  )\n",
    "print(  test_data.size()   )\n",
    "\n",
    "class three_layer_recurrent_net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(three_layer_recurrent_net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = nn.Transformer(nhead=16, num_encoder_layers=vocab_size)\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
    "\n",
    "        \n",
    "    def forward(self, word_seq, h_init ):\n",
    "        \n",
    "        g_seq               =   self.layer1( word_seq )  \n",
    "        h_seq , h_final     =   self.layer2( g_seq , h_init )\n",
    "        score_seq           =   self.layer3( h_seq )\n",
    "        \n",
    "        return score_seq,  h_final \n",
    "\n",
    "hidden_size=150\n",
    "\n",
    "net = three_layer_recurrent_net( hidden_size )\n",
    "net = net.to(device)\n",
    "\n",
    "def normalize_gradient(net):\n",
    "\n",
    "    grad_norm_sq=0\n",
    "\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "   \n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('grad norm close to zero')\n",
    "    else:    \n",
    "        for p in net.parameters():\n",
    "             p.grad.data.div_(grad_norm)\n",
    "\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    \n",
    "    h=h.to(device)\n",
    "\n",
    "       \n",
    "    for count in range( 0 , 1000-seq_length ,  seq_length) :\n",
    "               \n",
    "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "        \n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "                                  \n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "        \n",
    "        loss = criterion(  scores ,  minibatch_label )    \n",
    "        \n",
    "        h=h.detach()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "        \n",
    "start=time.time()\n",
    "\n",
    "for epoch in range(15):\n",
    "    \n",
    "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "    if epoch >= 4:\n",
    "        my_lr = my_lr / 1.1\n",
    "    \n",
    "    # create a new optimizer and give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h to be the zero vector\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "    # send it to the gpu    \n",
    "    h=h.to(device)\n",
    "    \n",
    "    for count in range( 0 , 1000-seq_length ,  seq_length):\n",
    "             \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # Detach to prevent from backpropagating all the way to the beginning\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.detach()\n",
    "        h=h.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    eval_on_test_set() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Implement the following bi-directional vanilla recurrent neural network :\n",
    "\n",
    "$$\n",
    "\\overrightarrow{h}_t = \\textrm{ReLU}( W_h[\\overrightarrow{h}_{t-1},x_t]+b_h)\\quad \\textrm{for } t=1,2,...,T \\\\\n",
    "\\overleftarrow{h}_t = \\textrm{ReLU}( W_h[\\overleftarrow{h}_{t+1},x_t]+b_h)\\quad \\textrm{for } t=T,T-1,...,1 \\\\\n",
    "s_t = W_s [\\overrightarrow{h}_{t},\\overleftarrow{h}_{t}]+b_s \\quad \\textrm{for } t=1,2,...,T\n",
    "$$\n",
    "\n",
    "where $[a,b]$ is the concatenation of tensors $a$ and $b$ along the feature dimension.\n",
    "\n",
    "Compute and print the value of the scores $s_t$ for $t=1,2,3$, $x_1=[1,0]$, $x_2=[0,1]$, $x_3=[0,1]$, the weights $W_h,W_s$ having all elements equal to 1, the biases $b_h,b_s$ having all elements equal to 0, the dimensionality of $\\overrightarrow{h},\\overleftarrow{h}$ being equal to 2, and $\\overrightarrow{h},\\overleftarrow{h}$ initialized with value equal to 0.\n",
    "\n",
    "Possible hints: Functions *torch.cat*, *.append()*, *torch.stack*\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-11-25--18-16-19\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "x1 = torch.Tensor([1,0])\n",
    "x2 = torch.Tensor([0,1])\n",
    "x3 = torch.Tensor([0,1])\n",
    "\n",
    "h_right = torch.Tensor([0,0]) \n",
    "h_left = torch.Tensor([0,0])  \n",
    "\n",
    "# YOUR CODE HERE\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(4, 2, bias=False)\n",
    "        self.output_linear = nn.Linear(4, 2, bias=False)\n",
    "    def forward(self, h_right, h_left, x1, x2, x3):\n",
    "        h_right = F.relu(self.hidden_linear(torch.cat([h_right, x1], dim=-1)))\n",
    "        h_right = F.relu(self.hidden_linear(torch.cat([h_right, x2], dim=-1)))\n",
    "        h_right = F.relu(self.hidden_linear(torch.cat([h_right, x3], dim=-1)))\n",
    "        h_left = F.relu(self.hidden_linear(torch.cat([h_left, x3], dim=-1)))\n",
    "        h_left = F.relu(self.hidden_linear(torch.cat([h_left, x2], dim=-1)))\n",
    "        h_left = F.relu(self.hidden_linear(torch.cat([h_left, x1], dim=-1)))\n",
    "        return self.output_linear(torch.cat([h_right, h_left], dim=-1))\n",
    "            \n",
    "net = BiRNN()\n",
    "s = net(h_right, h_left, x1, x2, x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNd30TjgREYWyAkV0lT4iX6",
   "name": "CE_7454_coding_test_solution_Vijay.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
